---
title: |
  Bayesian modeling of comparative judgment data with `R` and `Stan`
author:
  - name: 
      given: Jose(ma)
      family: Rivera
    orcid: 0000-0002-3088-2783
    url: https://www.uantwerpen.be/en/staff/jose-manuel-rivera-espejo_23166/
    email: JoseManuel.RiveraEspejo@uantwerpen.be
    corresponding: true
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Tine
      family: van Daal
    orcid: 0000-0001-9398-9775
    url: https://www.uantwerpen.be/en/staff/tine-vandaal/
    email: tine.vandaal@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Sven
      family: De Maeyer
    orcid: 0000-0003-2888-1631
    url: https://www.uantwerpen.be/en/staff/sven-demaeyer/
    email: sven.demaeyer@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Steven
      family: Gillis
    orcid: 
    url: https://www.uantwerpen.be/nl/personeel/steven-gillis/
    email: steven.gillis@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Linguistics
        group: Centre for computational linguistics, psycholinguistics, and sociolinguistics (CLiPS)
date: last-modified
bibliography: references.bib
title-slide-attributes: 
  data-notes: | 
    (to do) 
---

# 1. Introduction {style="font-size:80%;"}

---

## 1. Introduction {style="font-size:80%;"}

The Bradley-Terry-Luce (BTL) model [@Bradley_et_al_1952; @Luce_1959] offers a **simple method** for measuring traits and conducting statistical inference from comparative judgment (CJ) data [@Andrich_1978; @Pollitt_2012b]. 

[Its simplicity stems from two features:]{.fragment}

::: incremental 
::: {style="font-size:80%;"}
1. A reliance on an extensive set of simplifying assumptions about the traits, judges, and stimuli involved in CJ assessments [@Thurstone_1927b; @Bramley_2008],
2. The use of ad hoc procedures to handle inferences, including hypothesis testing [@Pollitt_2012b].
:::
:::

::: {.fragment}
However, recent studies question whether:

::: incremental 
::: {style="font-size:80%;"}
- These assumptions hold in modern CJ applications? [@Bramley_2008; @Kelly_et_al_2022; @Rivera_et_al_2025]
- The ad hoc procedures effectively fulfill their intended analytical purpose? [@Kelly_et_al_2022; @Rivera_et_al_2025]
:::
:::
:::

---

## 1. Introduction {style="font-size:80%;"}

To address these concerns, Rivera et al. [@Rivera_et_al_2025] proposed *The Information-Theoretical model for CJ*. The approach,

[1. Extends the general form of Thurstone's law of comparative judgment [@Thurstone_1927a; @Thurstone_1927b], combining Thurstone's core theoretical principles with key CJ assessment design features.]{.fragment style="font-size:80%;"}

[2. Enables the development of models tailored to the assumed data-generating process of the CJ system under study, thus:]{.fragment style="font-size:80%;"}

::: incremental 
::: {style="font-size:80%;"}
- Eliminating the need to rely on the simplifying assumptions of the BTL model,
- Removing the dependence on ad hoc hypothesis-testing procedures.
:::
:::

[Nevertheless, although approach has the potential to yield reliable trait estimates and accurate statistical inferences, **this promise still needs to be empirically tested**.]{.fragment style="font-size:80%;"} 

<!-- ######################################### -->

# 2. Research goals

---

## 2. Research goals {style="font-size:80%;"}

Thus, this study has **two overarching goals**:

::: {.fragment style="font-size:80%;"}
1. *To show how apply the Information-Theoretical model for CJ to a simulated dataset*,

    A **tutorial component**, offering detailed guidance on data simulation, prior and model specification, estimation, and interpretation using the software `R` and `Stan`.
:::

::: {.fragment style="font-size:80%;"}
Once a *sufficiently trustworthy model* is found in goal 1,

2. *Evaluate whether the approach yield reliable trait estimates and accurate statistical inferences*,

    A **model validation component** benchmarked against the classical BTL analysis.
:::

<!-- ######################################### -->

# 3. A tale of two analytical approaches {#sec-approaches}

---

## 3. A tale of two analytical approaches {style="font-size:80%;"}

CJ data can be analyzed under two analytical approaches:

::: incremental 
1. *The classical BTL analysis* (hereafter, **CBTL analysis**) [@Pollitt_2012a; @Pollitt_2012b] ,
    
    which relies on a sequence of separate methods to estimate traits and draw inferences,

2. *The Information-Theoretical model for CJ* (hereafter, **ITCJ analysis**) [@Rivera_et_al_2025],

    which employs a single, systematic, and integrated approach for the same two purposes.
:::

<!-- ######################################### -->

# 3.1 The CBTL analysis

---

## 3.1 The CBTL analysis {#sec-CBTL style="font-size:80%;"}

This approach relies on a sequence of separate methods, each with different purposes [@Pollitt_2012a; @Pollitt_2012b; @Jones_et_al_2019; @Boonen_et_al_2020; @Chambers_et_al_2022; @Bouwer_et_al_2023]:

::: incremental 
::: {style="font-size:80%;"}
1. **Apply the BTL model** to data, e.g.,
    
::: incremental 
::: {style="font-size:80%;"}
- To estimate the traits of the stimuli (mean and standard error)
- To generate the model residuals
:::
:::

2. **Generate summaries** or **apply (multilevel) regression** to the stimuli' mean trait estimates, e.g.,
    
::: incremental 
::: {style="font-size:80%;"}
- To aggregate the trait of the stimuli at the individual level
- To assess the variability and conduct inferences at the stimuli and individual levels
- To calculate correlation with other methods (a way of "concurrent validity") or conduct inferences.
:::
:::

3. **Generate summaries** or **apply (multilevel) regression** to the model residuals, e.g.,
    
::: incremental 
::: {style="font-size:80%;"}
- To aggregate the remaining variability at the judges level or to investigate bias
- To assess the remaining variability and conduct inferences about judges biases
- To conduct inferences, including *misfit* identification, for stimuli, individuals, and judges
:::
:::

:::
:::

<!-- ######################################### -->

# 3.2 The ITCJ analysis

--- 

## 3.2 The ITCJ analysis {#sec-ITCJ style="font-size:80%;"}

This approach applies a single, systematic, and integrated approach to estimate traits and conduct inferences [@Rivera_et_al_2025]. In broad terms, the approach:

::: incremental 
::: {style="font-size:80%;"}
1. Begins with the general CJ structure proposed by Rivera et al. [@Rivera_et_al_2025] (see next slide),
2. Adapts this structure to the assumed data-generating process of the CJ system under study,
3. Develops one or more *bespoke* statistical models for analyzing the CJ system.
4. Use the one or more statistical models to estimate traits and conduct inferences.
:::
:::

::: {.fragment}
With these steps a researcher can:

::: incremental 
::: {style="font-size:80%;"}
- Estimate the traits of the stimuli and individuals (full distribution)
- Assess the variability and conduct inferences at the stimuli and individual levels
- Estimate the biases of the judges and judgments (full distribution)
- Assess the variability and conduct inferences at the judgments and judges levels
- Conduct *oulier* identification for stimuli, individuals, judgments, and judges (akin to *misfit* identification)
:::
:::

:::

--- 

## 3.2 The ITCJ analysis {style="font-size:80%;" #sec-3.2}

<!-- commands for d-separation -->
\newcommand{\dsep}{\:\bot\:}
\newcommand{\ndsep}{\:\not\bot\:}
\newcommand{\cond}{\:|\:}

The general CJ structure proposed by Rivera et al. [@Rivera_et_al_2025] takes the following form:

::: {.fragment style="font-size:80%;"}

::: {#fig-cj15 layout-ncol=2 }

![](/figures/population_summary/CJ_TM_15.png){width=80%}

$$
\begin{aligned}
  O_{R} & := f_{O}(D_{R}, S, C) \\
  D_{R} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, X_{IA}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}, Z_{JK}, e_{JK}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) \\
  e_{I} & \dsep \{ e_{J}, e_{IA}, e_{JK} \} \\
  e_{J} & \dsep \{ e_{IA}, e_{JK} \} \\
  e_{IA} & \dsep e_{JK} 
\end{aligned}
$$

Comparative judgment model. *Left panel* illustrates the DAG. *Right panel * depicts the associated SCM.
:::

:::

---

## 3.2 The Information-Theoretical model for CJ {style="font-size:80%;"}

Leading to the general probabilistic and statistical model:

::: {.fragment style="font-size:80%;"}

::: {#fig-cj16a layout-ncol=3}

$$
\begin{aligned}
  O_{R} & := f_{O}(D_{R}, S, C) \\ 
  D_{R} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, X_{IA}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}, Z_{JK}, e_{JK}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) \\ \\
  e_{I} & \dsep \{ e_{J}, e_{IA}, e_{JK} \} \\
  e_{J} & \dsep \{ e_{IA}, e_{JK} \} \\
  e_{IA} & \dsep e_{JK}
\end{aligned}
$$

$$
\begin{aligned}
  & P( O_{R} \mid D_{R}, S, C ) \\
  & P( D_{R} \mid T_{IA}, B_{JK} ) \\
  & P( T_{IA} \mid T_{I}, X_{IA}, e_{IA} ) \\
  & P( T_{I} \mid X_{I}, e_{I} ) \\
  & P( B_{JK} \mid B_{J}, Z_{JK}, e_{JK} ) \\
  & P( B_{J} \mid Z_{J}, e_{J} ) \\ \\
  & P( e_{I} ) P( e_{IA} ) P( e_{J} ) P( e_{JK} ) \\ \\ \\
\end{aligned}
$$

$$
\begin{aligned}
  O_{R} & \overset{iid}{\sim} \text{Bernoulli} \left[ \text{inv_logit}( D_{R} ) \right] \\
  D_{R} & = \left( T_{IA}[i,a] - T_{IA}[h,b] \right) + B_{JK}[j,k] \\
  T_{IA} & = T_{I} + \beta_{XA} X_{IA} + e_{IA} \\
  T_{I} & = \beta_{XI} X_{I} + e_{I} \\
  B_{JK} & = B_{J} + \beta_{ZK} Z_{JK} + e_{JK} \\
  B_{J} & = \beta_{ZJ} Z_{J} + e_{J} \\ \\
  \boldsymbol{e} & \sim \text{Multi-Normal}( \boldsymbol{\mu}, \boldsymbol{\Sigma} )
  \\
  \boldsymbol{\Sigma} &= \boldsymbol{V} \boldsymbol{Q} \boldsymbol{V} \\ \\
\end{aligned}
$$

Comparative judgment model, assuming different discriminal dispersions for traits. *Left panel* illustrates the SCM. *Middle panel* shows the probabilistic model. *Right panel* illustrates the statistical model. 
:::

:::


::: {.fragment}
With the following constraints to solve indeterminacies in *location*, *orientation*, and *scale* of $T_{I}$, $T_{IA}$, $B_{J}$, and $B_{JK}$ [@Depaoli_2021]: 
:::

::: {.fragment style="font-size:80%;"}

::: {#fig-cj16b}
$$
\boldsymbol{\mu} = [0, 0, 0, 0]^{T}; \quad 
\boldsymbol{Q} = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 
  \end{bmatrix}; \quad 
  \boldsymbol{V} = \begin{bmatrix}
    s_{XI} & 0 & 0 & 0 \\
    0 & s_{A} & 0 & 0 \\
    0 & 0 & s_{ZJ} & 0 \\
    0 & 0 & 0 & s_{K} 
  \end{bmatrix}; \quad 
  \sum_{g=1}^{3} s_{XI}[g]/3 = 1; \quad
  0< s_{A} < 1; \quad 
  \sum_{g=1}^{3} s_{ZJ}[g]/3 = 1; \quad
  0< s_{K} < 1
$$ 

Constraints of the CJ model to solve indeterminacies in *location*, *orientation*, and *scale* of $T_{I}$, $T_{IA}$, $B_{J}$, and $B_{JK}$.
:::
:::

<!-- ######################################### -->

# 4. Methods 

---

## 4. Methods {style="font-size:80%;"}

To meet the tutorial and model validation goals, this study follows the Bayesian (research) workflow [@Depaoli_et_al_2017; @Neal_2020; @Gelman_et_al_2020; @Schad_et_al_2020; @Betancourt_2020; @McElreath_2024b; @McElreath_2024c]:

::: {#fig-workflow}
![](/figures/workflow.png){width=80%}

Bayesian (research) workflow.
:::

---

## 4. Methods {#sec-methods style="font-size:80%;"}

Specifically, the study follows these steps (overarching goals in parenthesis):

::: incremental 
::: {style="font-size:75%;"}
1. **Theory $\rightarrow$ Estimand(s) $\rightarrow$ Design** (*Tutorial and Model validation*)

    Considering three steps: 

::: incremental 
a. Define a CJ structure of interest and explicitly state its assumptions from relevant literature; 
b. Specify the estimand(s) of interest and provide formal definitions for each target parameter;
c. Simulate a _synthetic conceptual population_ that reflects the defined structure and assumptions;
:::

2. **Design $\rightarrow$ Sample** (*Tutorial and Model validation*)
    
    Generate _**two** synthetic_ random sample and comparison datasets from the conceptual population simulated in step $1$;

3. **{Theory, Design, Estimand(s)} $\rightarrow$ Estimator** (*Tutorial and Model validation*)

    Specify models for analyzing the _**first** synthetic_ random comparison dataset using both the CBTL and the ITCJ analysis;
    
4. **Estimator $\rightarrow$ Prior predictive** (*Tutorial*)

    Perform prior predictive checks;
:::
:::

---

## 4. Methods {style="font-size:80%;"}

::: incremental 
::: {style="font-size:75%;"}
5. **{Estimator, Sample} $\rightarrow$ Estimate(s)** (*Tutorial and Model validation*)

    Apply both the CBTL and ITCJ methods to the _**first** synthetic_ random comparison dataset;

6. **Estimate(s) $\rightarrow$ {Diagnostic, Post predictive}** (*Tutorial*)

    Assess the quality of the models and estimate(s) in terms of stationarity, convergence, mixing, parameter recovery, in-sample fit, approximate out-of-sample fit, and in-sample predictive accuracy;
    
7. **{Diagnostic, Post predictive} $\rightarrow$ Estimator** (*Tutorial*)

    Incrementally refine the statistical model repeating steps 3–6 until a *sufficiently trustworthy model* is obtained according to the criteria outlined in step 6;

8. **Estimator $\rightarrow$ Estimate(s) $\rightarrow$ Effects $\leftarrow$ Estimand(s)** (*Model validation*)

    Generate the *estimate(s)* of interest for the _**first** synthetic_ random comparison dataset using both the CBTL and ITCJ analysis, and interpret the results;
    
9. **Estimator $\rightarrow$ Estimate(s) $\rightarrow$ Predictions $\leftarrow$ Estimand(s)** (*Model validation*)

    Generate predictions for the _**second** synthetic comparison dataset_ using both the CBTL and ITCJ analysis, and compare their out-of-sample predictive accuracy.
:::
:::


<!-- ######################################### -->

# 4.1 From Theory to Design: Steps 1a-1c

---

## 4.1 From Theory to Design: Steps 1a-1c {style="font-size:80%;"}

The conceptual population simulation is based on the data characteristics and findings reported by Boonen et al. [@Boonen_et_al_2020]:

::: {.fragment style="font-size:80%;"}
Regarding the data characteristics, the study:

::: incremental 
::: {style="font-size:80%;"}
- Includes multiple stimuli nested within multiple individuals,
- Considers individuals with different characteristics (e.g., age, hearing status),
- Assigns each judge to make only one comparison per stimulus pair,
- Involves judges who differ in characteristics (e.g., experience level),
- Selects stimuli, individuals, and judges using a (pseudo-)random sampling algorithm,
- Involves multiple judges performing multiple comparisons, assigned through a random comparison algorithm.
:::
:::

:::

::: {.fragment style="font-size:80%;"}
Regarding the study findings, the study report that:

::: incremental 
::: {style="font-size:80%;"}
- Individuals with different hearing statuses differ in both their average latent trait levels and their **variability**,
- There is more unexplained variability between individuals than within individuals (i.e., at the stimulus level),
- No evidence of systematic judge bias was found (although it was not tested but treated as a model assumption),
- Judges' experience levels did not account for differences in mean latent traits between stimuli, but differences in their **variabilities** were not assessed.
:::
:::

:::

---

## 4.1 From Theory to Design: Steps 1a-1c {style="font-size:80%;"}

Thus, adapting the general CJ structure proposed by Rivera et al.'s [@Rivera_et_al_2025] to the data characteristics reported by Boonen et al. [@Boonen_et_al_2020] leads to the following conceptual population data-generating process:

::: {.fragment style="font-size:80%;"}

::: {#fig-cj17 layout-ncol=2}

![](/figures/data_summary/CJ_population_DAG.png){width=85%}

$$
\begin{aligned}
  O_{R} & := f_{O}(D_{R}, S, C) \\
  D_{R} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) \\ \\
  e_{I} & \dsep \{ e_{J}, e_{IA} \} \\
  e_{J} & \dsep \{ e_{IA} \}
\end{aligned}
$$

CJ data-generating process for the conceptual population. *Left panel* shows the DAG. *Right panel * depicts the associated SCM.
:::

:::

---

## 4.1 From Theory to Design: Steps 1a-1c {style="font-size:80%;"}

Moreover, integrating the assumptions derived from Boonen et al. [@Boonen_et_al_2020] leads to the following statistical data-generating process:

::: {.fragment style="font-size:80%;"}

::: {#fig-cj18a layout-ncol=3}

$$
\begin{aligned}
  O_{R} & := f_{O}(D_{R}, S, C) \\
  D_{R} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) \\ \\
  e_{I} & \dsep \{ e_{J}, e_{IA} \} \\
  e_{J} & \dsep \{ e_{IA} \}
\end{aligned}
$$

$$
\begin{aligned}
  & P( O_{R} \mid D_{R}, S, C ) \\
  & P( D_{R} \mid T_{IA}, B_{JK} ) \\
  & P( T_{IA} \mid T_{I}, e_{IA} ) \\
  & P( T_{I} \mid X_{I}, e_{I} ) \\
  & P( B_{JK} \mid B_{J} ) \\
  & P( B_{J} \mid Z_{J}, e_{J} ) \\ \\
  & P( e_{I} ) P( e_{IA} ) P( e_{J} )
\end{aligned}
$$

$$
\begin{aligned}
  O_{R} & \overset{iid}{\sim} \text{Bernoulli} \left[ \text{inv_logit}( D_{R} ) \right] \\
  D_{R} & = \left( T_{IA}[i,a] - T_{IA}[h,b] \right) + B_{JK}[j,k] \\
  T_{IA} & = T_{I} + e_{IA} \\
  T_{I} & = \beta_{XI} X_{I} + e_{I} \\
  B_{JK} & = B_{J} \\
  B_{J} & = \beta_{ZJ} Z_{J} + e_{J} \\ \\
  \boldsymbol{e} & \sim \text{Multi-Normal}( \boldsymbol{\mu}, \boldsymbol{\Sigma} )
  \\
  \boldsymbol{\Sigma} &= \boldsymbol{V} \boldsymbol{Q} \boldsymbol{V}
\end{aligned}
$$

Data-generating process for simulated CJ data. *Left panel* illustrates the SCM. *Middle panel* shows the probabilistic model. *Right panel* illustrates the statistical model.
:::

:::

::: {.fragment}
With the following parameter assumptions:
:::

::: {.fragment style="font-size:80%;"}
::: {#fig-cj18b layout-ncol=2 }

$$
\begin{split}
X_{I} &= \{ X_{Ic}, X_{Id}[g=1], X_{Id}[g=2], X_{Id}[g=3] \} \\
Z_{J} &= \{ Z_{Jd}[g=1], Z_{Jd}[g=2], Z_{Jd}[g=3] \} \\
\beta_{XI} & = \{ \beta_{XIc}, \beta_{XId[g=1]}, \beta_{XId[g=2]}, \beta_{XId[g=3]} \} = \{ 0.1, 1, 0, -1\} \\
\beta_{ZJ} &= \{ \beta_{ZJd[g=1]}, \beta_{ZJd[g=2]}, \beta_{ZJd[g=3]} \} = \{ 0, 0, 0\}
\end{split}
$$

$$
\begin{split}
s_{XI[g]} &= \{ s_{XId[g=1]}, s_{XId[g=2]}, s_{XId[g=3]} \} = \{ 1.5, 0.75, 0.75\} \\
s_{ZJ} &= \{ s_{ZJd[g=1]}, s_{ZJd[g=2]}, s_{ZJd[g=3]} \} = \{ 0.5, 1, 1.5\} \\
s_{A} &= 0.2 \\
\boldsymbol{\mu} &= [0, 0, 0]^{T}; \quad 
\boldsymbol{Q} = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
\end{bmatrix} ;
\boldsymbol{V} = \begin{bmatrix}
    s_{XI} & 0 & 0 \\
    0 & s_{A} & 0 \\
    0 & 0 & s_{ZJ} 
\end{bmatrix}
\end{split}
$$

Simulating parameter assumptions.
:::
:::


---

## 4.1 From Theory to Design: Steps 1a-1c {style="font-size:80%;"}

In layman terms:

::: incremental 
::: {style="font-size:60%;"}
- No stimuli characteristics $(X_{IA})$ are assumed to affect the comparisons;
- Stimuli latent trait residual variability is smaller than average individual latent trait residual variability $(s_{A} = 0.2 < 1)$;
- Individual characteristics $(X_{I})$ include a continuous variable $(X_{Ic})$ for the age of children, and a categorical variable with three levels representing hearing status groups: normal-hearing (NH, $X_{Id}[g=1]$), hearing-impaired with hearing aids (HI-HA, $X_{Id}[g=2]$), and hearing-impaired with cochlear implants (HI-CI, $X_{Id}[g=3]$) children;
- There is a "small" [@Cohen_1988; @Sawilowsky_2009] but increasing effect of age $(\beta_{XIc})$ on the mean latent trait of individuals;
- There are "very large" [@Cohen_1988; @Sawilowsky_2009] differences in the mean latent trait across hearing status groups $(\beta_{XId[g=1]} > \beta_{XId[g=2]} > \beta_{XId[g=3]})$;
- Individual latent trait residual variability differs by groups, i.e., $s_{XId[g=1]} > s_{XId[g=2]} = s_{XId[g=3]}$, with $\sum_{g=1}^{3} s_{XId[g]}/3 = 1$;
- Judges make only one comparison; thus, there are no judgment-level characteristics $(Z_{JK})$, judgement-level effects $(\beta_{ZJK})$, or residual judgment variability $(p_{JK})$;
- Judges characteristics $(Z_{J})$ include a categorical variable with three levels representing judge groups: audiologist (AU, $Z_{Jd}[g=1]$), primary teachers (PT, $Z_{Jd}[g=2]$), and inexperienced listeners (IL, $Z_{Jd}[g=3]$);
- Judges' mean latent biases across judge groups are equal to zero $(\beta_{ZJd[g=1]}=\beta_{ZJd[g=2]}=\beta_{ZJd[g=3]}=0)$;
- However, judges exhibit more or less bias depending on their experience, i.e., $(s_{ZJd[g=1]}=0.5) < (s_{ZJd[g=2]} = 1) < (s_{ZJd[g=3]} = 1.5)$, with $\sum_{g=1}^{3} s_{ZJ[g]}/3 = 1$;
- Residual latent errors $(\boldsymbol{e})$ are centered around zero with zero correlation $(\boldsymbol{\mu}, \boldsymbol{Q})$.
:::
:::


---

## 4.1 From Theory to Design: Steps 1a-1c {#sec-pop_sim style="font-size:80%;"}

From the simulation parameters we can define the **_estimands_** of interest:

::: incremental 
::: {style="font-size:65%;"}
- The conditioned expected change in the individuals' mean latent trait for one additional year of age $(\beta_{XIc})$;
- The conditioned expected differences in mean latent traits between NH and HI-HA children $(\beta_{XId[g=1]} - \beta_{XId[g=2]})$, NH and HI-CI children $(\beta_{XId[g=1]} - \beta_{XId[g=3]})$, and HI-HA versus HI-CI children $(\beta_{XId[g=2]} - \beta_{XId[g=3]})$;
- The conditioned expected differences in mean latent bias between AU and PT judges $(\beta_{ZJd[g=1]} - \beta_{ZJd[g=2]})$, AU and IL judges $(\beta_{ZJd[g=1]} - \beta_{ZJd[g=3]})$, and PT and IL judges $(\beta_{ZJd[g=2]} - \beta_{ZJd[g=3]})$;
- The latent trait residual variability of NH, HI-HA, and HI-CI children, i.e., $s_{XId[g=1]}$, $s_{XId[g=2]}$, and $s_{XId[g=3]}$, respectively;
- The conditioned expected differences in residual variability of the latent trait between NH and HI-HA children $(s_{XId[g=1]} - s_{XId[g=2]})$, NH and HI-CI children $(s_{XId[g=1]} - s_{XId[g=3]})$, and HI-HA versus HI-CI children $(s_{XId[g=2]} - s_{XId[g=3]})$;
- The latent bias residual variability of AU, PT, and IL judges, i.e., $s_{ZJd[g=1]}$, $s_{ZJd[g=2]}$, and $s_{ZJd[g=3]}$, respectively;
- The conditioned expected differences in residual variability of the latent bias between AU and PT judges $(s_{ZJd[g=1]} - s_{ZJd[g=2]})$, AU and IL judges $(s_{ZJd[g=1]} - s_{ZJd[g=3]})$, and PT and IL judges $(s_{ZJd[g=2]} - s_{ZJd[g=3]})$;
- The residual variability of the stimuli latent trait $(s_{A})$;
- Latent traits $T_{IA}$, $T_{I}$, and $B_{J}$.
:::
:::

<!-- ######################################### -->

# 4.2 From Design to Sample: Step 2

---

## 4.2 From Design to Sample: Step 2 {style="font-size:80%;"}

Differing only by replication seed, the study generates _**two** synthetic_ random sample and comparison datasets from the conceptual population in **Step 1**. 

::: {.fragment}
More specifically, for the sampling $(S)$ and comparison $(C)$ mechanisms illustrated in @fig-cj17 sample size calculations were conducted (see @sec-AppA), and the following design was adopted:

::: incremental 
::: {style="font-size:80%;"}
- A sample of $54$ individuals, divided into three hearing status groups: $40$ NH $(X_{Id}[g=1])$, $7$ HI-HA $(X_{Id}[g=2])$, and $7$ HI-CI $(X_{Id}[g=3])$ children;
- A sample of $10$ stimuli per individual;
- A sample of $60$ judges, divided into three groups: $10$ AU $(Z_{Jd}[g=1])$, $10$ PT $(Z_{Jd}[g=2])$, and $40$ IL $(Z_{Jd}[g=3])$;
- Judges conduct only one-comparison of the same stimulus pair (i.e., design is NOT a *repeated measures design* [@Lawson_2015, chap. 9.5])
- Each stimulus is compared $20$ times in total against other stimuli, across all judges.
:::
:::

:::

<!-- ######################################### -->

# 4.3 From Estimator and Sample to Estimate(s): The analysis approaches and software of step 5

---

## 4.3 From Estimator and Sample to Estimate(s): The analysis approaches and software of step 5 {style="font-size:80%;"}

The study applies two data analysis approaches for the simulated CJ data (as described in @sec-approaches):

::: incremental 
1. *The CBTL analysis* [@Pollitt_2012a; @Pollitt_2012b] ,
2. *The ITCJ analysis* [@Rivera_et_al_2025],
:::

::: {.fragment}
Both approaches were conducted using `R` version 4.2.2 [@R_2015], with:

::: incremental 
::: {style="font-size:80%;"}
- Additional `R` packages for data manipulation and visualization: `tidyverse` [@Wickham_et_al_2019], `igraph` [@Csardi_et_al_2006; @Csardi_et_al_2025], and `RColorBrewer` [@Neuwirth_2022];
- Specific user-defined functions (`UDFs`) to facilitate data manipulation, visualization, model summarization, diagnostics, and prediction from the approaches (all are provided in the main document).
:::
:::

:::

---

## 4.3 From Estimator and Sample to Estimate(s): The analysis approaches and software of step 5 {style="font-size:80%;"}

Specifically, the *CBTL analysis* [@Pollitt_2012a; @Pollitt_2012b; @Jones_et_al_2019; @Boonen_et_al_2020; @Chambers_et_al_2022; @Bouwer_et_al_2023; @Thwaites_et_al_2024]:

::: incremental 
::: {style="font-size:80%;"}
1. **Fits a BTL model** using the `BTm()` function from the `BradleyTerry2` package [@Turner_et_al_2012a; @Turner_et_al_2012b],

::: incremental 
::: {style="font-size:80%;"}
- To estimate the traits of the stimuli $(T_{IA})$
- To generate the model residuals and conduct *misfit* analysis.
:::
:::

2. **Fits a (multilevel) regression** to model residuals using the `brms` package [@Burkner_2017; @Burkner_2018],
    
::: incremental 
::: {style="font-size:80%;"}
- To test hypothesis of differences in mean residuals between groups of judges, i.e., $\beta_{ZJd[g=1]} - \beta_{ZJd[g=2]}$, $\beta_{ZJd[g=1]} - \beta_{ZJd[g=3]}$, and $\beta_{ZJd[g=2]} - \beta_{ZJd[g=3]}$;
- To assess the variability between- and within-judges, where the latter has **no direct equivalent parameter**, while former corresponds to $s_{ZJ}$$^{*}$.
- To aggregate model residuals into judges biases $(B_{J})$ 
:::
:::

:::
:::

:::{.fragment style="font-size:60%; color:gray"}
$*$ By model assumptions, the CBTL analysis only considers one variability parameter for judges, i.e., $s_{ZJ}$, versus multiple variability parameters defined by the experience of the judges, i.e., $s_{ZJd}$.
:::

---

## 4.3 From Estimator and Sample to Estimate(s): The analysis approaches and software of step 5 {style="font-size:80%;"}

Specifically, the *CBTL analysis* [@Pollitt_2012a; @Pollitt_2012b; @Jones_et_al_2019; @Boonen_et_al_2020; @Chambers_et_al_2022; @Bouwer_et_al_2023; @Thwaites_et_al_2024]:

::: incremental 
::: {style="font-size:80%;"}
3. **Fits a (multilevel) regression** to the stimuli' mean trait estimates, 
    
::: incremental 
::: {style="font-size:80%;"}
- To estimate the effect of age on the mean latent trait, i.e, $\beta_{XIc}$
- To test hypothesis of differences in mean latent traits between groups of children, i.e., $\beta_{XId[g=1]} - \beta_{XId[g=2]}$, $\beta_{XId[g=1]} - \beta_{XId[g=3]}$, and  $\beta_{XId[g=2]} - \beta_{XId[g=3]}$
- To assess the variability within and between children, i.e., $s_{A}$ and $s_{XI}$$^{*}$, respectively
- To aggregate stimuli traits into individual level $(T_{I})$ 
:::
:::

:::
:::

::: {.fragment style="font-size:80%;"}
Notably, **for each** `brms` model, **four** Markov chains of $4000$ iterations were run, each with distinct starting values. The first $2000$ iterations served as warm-up, and the remaining $2000$ were used as posterior samples (for a total of $8000$ posterior samples).
:::

:::{.fragment style="font-size:60%; color:gray"}
$*$ By model assumptions, the CBTL analysis only considers one variability parameter for individuals, i.e., $s_{XI}$, versus multiple variability parameters defined by the individuals' groups, i.e., $s_{XId}$.
:::


---

## 4.3 From Estimator and Sample to Estimate(s): The analysis approaches and software of step 5 {style="font-size:80%;"}

In contrast, for the *ITCJ analysis* [@Rivera_et_al_2025], the study uses:

::: incremental 
::: {style="font-size:80%;"}
- `Stan` version 2.26.1 [@Stan_2020] and the interface package `cmdstanr` [@Gabry_et_al_2025] to fit a series of increasingly complex Bayesian ITCJ models,
- Additional `R` packages for model summaries, predictions, and diagnostics: `loo` [@Vehtari_et_al_2024b], `posterior` [@Burkner_et_al_2024], and `bayesplot` [@Gabry_et_al_2019; @Gabry_et_al_2025].

:::
:::

::: {.fragment style="font-size:80%;"}
Notably, **for each** Bayesian model, **four** Markov chains of $4000$ iterations were run, each with distinct starting values. The first $2000$ iterations served as warm-up, and the remaining $2000$ were used as posterior samples (for a total of $8000$ posterior samples).
:::

<!-- ######################################### -->

# 4.4 From Estimate(s) to Diagnostics and Post predictive: The evaluation criteria for step 6

---

## 4.4 From Estimate(s) to Diagnostics and Post predictive: The evaluation criteria for step 6 {style="font-size:80%;"}

The study assess the quality of the models and estimate(s) in terms of:

::: incremental 
::: {style="font-size:80%;"}
1. **_Stationarity, convergence, and mixing_** (for Bayesian models only), using

::: incremental 
::: {style="font-size:80%;"}
- Graphical analyses, including trace plots, rank-normalized trace (trank) plots, autocorrelation function (ACF) plots, and comparison plots of prior to posterior distributions,
- Diagnostic statistics, including the potential scale reduction factor statistics $(\hat{R})$ with a cut-off value of $1.05$ [@Vehtari_et_al_2021a] and effective sample size statistics $(n_{\text{eff}})$ [@Gelman_et_al_2014].
:::
:::

2. **_Parameter recovery_**, using

::: incremental 
::: {style="font-size:80%;"}
- The graphical comparisons of "true" parameters values versus posterior estimates,
- The parameter posterior Root Mean Squared Error $(\text{RMSE})$, defined as follows:
:::
:::
    
:::
:::

::: {.fragment}
$$
\text{RMSE}( \boldsymbol{\hat{\theta}}, \theta) = \sqrt{ \frac{1}{S} \sum_{s=1}^{S} ( \hat{\theta}_{s} - \theta )^2 }
$$
:::

::: {.fragment style="font-size:64%;"}
where $\boldsymbol{\hat{\theta}}$ is the vector of posterior samples associated with the "true" parameter $\theta$, and $\hat{\theta}_{s}$ is the $s$-th sample out of a total of $S$ posterior draws.
:::

---

## 4.4 From Estimate(s) to Diagnostics and Post predictive: The evaluation criteria for step 6 {style="font-size:80%;"}

The study assess the quality of the models and estimate(s) in terms of:

::: incremental 
::: {style="font-size:80%;"}
3. **_In-sample fit_**, using

::: incremental 
::: {style="font-size:80%;"}
- The deviance information criterion $(\text{DIC})$ [@Spiegelhalter_et_al_2002]
:::
:::

4. **_Approximate out-of-sample fit_**, using

::: incremental 
::: {style="font-size:80%;"}
- The widely applicable information criterion $(\text{WAIC})$ [@Watanabe_2013] and its standard error $(\text{SE}_{W})$, along with the differences in $\text{WAIC}$ between models $(\text{dWAIC} \pm 1 \cdot \text{dSE}_{W})$
- The Pareto Smoothing Importance Sampling criterion $(\text{PSIS})$ [@Vehtari_et_al_2017; @Vehtari_et_al_2024a] and its standard error $(\text{SE}_{P})$, along with the differences in $\text{PSIS}$ between models $(\text{dPSIS} \pm 1 \cdot \text{dSE}_{P})$
:::
:::

5. **_In-sample predictive accuracy_**, using

::: incremental 
::: {style="font-size:80%;"}
- Confusion matrix comparing expected posterior predictions $E( \boldsymbol{\hat{y}})$ with observed outcomes $\boldsymbol{y}$ from the **_first_** synthetic dataset, both non-aggregated and aggregated at the levels of stimuli, individuals, and judges,
- (Multiple) confusion matrix comparing the posterior predictions $\boldsymbol{\hat{y}}_{s}$ with observed outcomes  $\boldsymbol{y}$ from the **_first_** synthetic dataset, both non-aggregated and aggregated at the levels of stimuli, individuals, and judges,
:::
:::

:::
:::

---

## 4.4 From Estimate(s) to Diagnostics and Post predictive: The evaluation criteria for step 6 {style="font-size:80%;"}

The study assess the quality of the models and estimate(s) in terms of:

::: incremental 
::: {style="font-size:80%;"}
6. **_Out-of-sample predictive accuracy_**, using

::: incremental 
::: {style="font-size:80%;"}
- Confusion matrix comparing expected posterior predictions $E( \boldsymbol{\hat{y}})$ with observed outcomes $\boldsymbol{y}$ from the **_second_** synthetic dataset, both non-aggregated and aggregated at the levels of stimuli, individuals, and judges,
- (Multiple) confusion matrix comparing the posterior predictions $\boldsymbol{\hat{y}}_{s}$ with observed outcomes  $\boldsymbol{y}$ from the **_second_** synthetic dataset, both non-aggregated and aggregated at the levels of stimuli, individuals, and judges,
:::
:::

:::
:::


<!-- ######################################### -->

# 5. Results

---

## 5. Results {style="font-size:80%;"}

In this section, the study will:

::: incremental 
1. Describe the _**first** synthetic_ random comparison dataset;
2. Progress through the steps $3-7$ of the Bayesian (research) workflow, described in @sec-methods, using:

::: incremental 
- The CBTL analysis, and 
- The ITCJ analysis
:::

:::

<!-- ######################################### -->

# 5.1 The first synthetic comparison dataset

# 5.1.1 Data description

---

## 5.1.1 Data description {style="font-size:80%;"}

In terms of design, the dataset reveals that:

::: incremental 
::: {style="font-size:80%;"}
- Most stimuli were compared $20$ times; only two stimuli (IDs $2$ and $3$) from individual $58$ were compared slightly fewer times due to random variation.
- The stimuli comparison network indicates a random *balanced design* [@Lawson_2015, chap. 7.4].
:::
:::

::: {.fragment style="font-size:80%;"}
::: {#fig-stimuli_comparisons layout-ncol=2}
![](/figures/data_summary/stimuli_comparisons_bottom.png){width=60%}

![](/figures/data_summary/stimuli_network.png){width=60%}

Comparison design. *Left panel* shows the number of comparisons for individuals $(Is)$ and stimuli $(As)$. *Right panel* shows the stimuli comparison network.
:::
:::

---

## 5.1.1 Data description {style="font-size:80%;"}

In a similar manner, the data indicates:

::: incremental 
::: {style="font-size:80%;"}
- Most individuals were compared $200$ times ($20$ comparisons × $10$ stimuli each); only one individual (ID $58$) was compared slightly fewer times due to random design variation;
- The connected component analysis [@Betancourt_2024] and individual comparison network indicates a fully connected network and a *balanced design* for individuals [@Lawson_2015, chap. 7.4].
:::
:::

::: {.fragment style="font-size:80%;"}
::: {#fig-individual_comparisons layout-ncol=2}

![](/figures/data_summary/individuals_comparisons.png){width=60%}

![](/figures/data_summary/individual_network.png){width=55%}

Comparison design. *Left panel* shows the number of comparisons for individuals. *Right panel* shows the individual comparison network.
:::
:::

---

## 5.1.1 Data description {style="font-size:80%;"}

On the other hand, the dataset shows:

::: incremental 
::: {style="font-size:80%;"}
- Judges compare individuals with frequencies ranging from $0$ to $13$ comparisons;
- Most judges completed $262$ comparisons, while a few completed $264$ due to random design variation.
:::
:::

::: {.fragment style="font-size:80%;"}
::: {#fig-judges_comparisons layout-ncol=2}
![](/figures/data_summary/judges2individuals_comparisons.png){width=60%}

![](/figures/data_summary/judges_comparisons.png){width=90%}

Comparison design. *Left panel* shows the judges $(Js)$ versus the first $10$ individuals $(Is)$. *Right panel* shows the total number of judges' comparisons.
:::
:::

---

## 5.1.1 Data description {style="font-size:80%;"}

Moreover,

::: incremental 
::: {style="font-size:80%;"}
- Judges to individual comparison network indicates a fully connected network
:::
:::

::: {.fragment style="font-size:80%;"}
::: {#fig-judges_individuals_comparisons}
![](/figures/data_summary/judges_individuals_network.png){width=100%}

Bipartite graph of judges to individual comparison network.
:::
:::

---

## 5.1.1 Data description {style="font-size:80%;"}

In terms of the comparison outcomes, we see some stimuli with higher win rates than others:

::: {.fragment style="font-size:80%;"}
::: {#fig-stimuli_wins}
![](/figures/data_summary/stimuli_wins.png){width=70%}

Stimuli win rates.
:::
:::

---

## 5.1.1 Data description {style="font-size:80%;"}

Aggregated by individuals, we see some individuals with higher win rates than others:

::: {.fragment style="font-size:80%;"}
::: {#fig-stimuli_wins}
![](/figures/data_summary/individual_wins.png){width=70%}

Individual win rates.
:::
:::

---

## 5.1.1 Data description {style="font-size:80%;"}

Divided by hearing status groups, 

::: incremental 
::: {style="font-size:80%;"}
- It is harder to see average differences between the groups,
- However, we can easily notice the different variability between them
:::
:::

::: {.fragment style="font-size:80%;"}
::: {#fig-stimuli_wins}
![](/figures/data_summary/individual_wins_groups.png){width=100%}

Individual win rates per group. *Left panel* describe NH children. *Middle panel* describe HI-HA children. *Right panel* illustrate HI-CI children.
:::
:::

---

## 5.1.1 Data description {style="font-size:80%;"}

However, no apparent relationship transpire between individual wins and age:

::: {.fragment style="font-size:80%;"}
::: {#fig-stimuli_wins}
![](/figures/data_summary/individual_winsVSXIc.png){width=70%}

Individual win rates versus age.
:::
:::


---

## 5.1.1 Data description {style="font-size:80%;"}

Considering the interaction of age and hearing status groups, 

::: incremental 
::: {style="font-size:80%;"}
- We notice a slightly decreasing trend in HI-HA and HI-CI children, indicating that in those groups, older children are less likely to win in a comparison, but the results are not *unambiguous* (*Simpson's, Berkson's or another paradox*?).
:::
:::

::: {.fragment style="font-size:80%;"}
::: {#fig-stimuli_wins}
![](/figures/data_summary/individual_winsVSXIc_groups.png){width=100%}

Individual win rates.
:::
:::

<!-- ######################################### -->

# 5.1.2 Data modeling

# 5.1.2.1 Data modeling: The CBTL analysis

---

## 5.1.2.1 Data modeling: The CBTL analysis {style="font-size:80%;"}




<!-- ######################################### -->

# 5.1.2.2 Data modeling: The ITCJ analysis 

---

## 5.1.2.2 Data modeling: The ITCJ analysis {style="font-size:80%;"}

<!-- ######################################### -->

# 5.2 The second synthetic comparison dataset

# 5.2.1 Data description

---

## 5.2.1 Data description {style="font-size:80%;"}

<!-- ######################################### -->

# 5.2.2 From Estimands and Estimator to Effects and Predictions: Steps 8 and 9

---

## 5.2.2 From Estimands and Estimator to Effects and Predictions: Steps 8 and 9 {style="font-size:80%;"}

<!-- ######################################### -->

# 5.2.2.1 The classical BTL analysis

---

## 5.2.2.1 The classical BTL analysis {style="font-size:80%;"}

<!-- ######################################### -->

# 5.2.2.2 The Information-Theoretical model for CJ

---

## 5.2.2.2 The Information-Theoretical model for CJ {style="font-size:80%;"}

<!-- ######################################### -->


# 6. Discussion

---

## 6. Discussion {style="font-size:80%;"}


<!-- ######################################### -->

# 6.1 Future research directions

---

## 6.1 Future research directions {style="font-size:80%;"}

<!-- ######################################### -->

# 6.2 Study limitations

---

## 6.2 Study limitations {style="font-size:80%;"}


<!-- ######################################### -->

# 7. Conclusion

---

## 7. Conclusion {style="font-size:80%;"}

<!-- ######################################### -->

---

# Appendix A - From Design to Sample: Step 2 {#sec-AppA}

---

## Appendix A - From Design to Sample: Step 2 {style="font-size:80%;"}

Differing only by replication seed, the study generates _**two** synthetic_ random sample and comparison datasets from the conceptual population in **Step 1**. 

::: {.fragment}
More specifically, for the sampling $(S)$ and comparison $(C)$ mechanisms shown in @fig-cj17, sample size calculations were conducted assuming:

::: incremental 
::: {style="font-size:80%;"}
- That "reaching" one children from the HI-HA or HI-CI groups costs ten times $(10x)$ more that "reaching" one NH child.
- Three criteria for individual sample size selection: (1) a minimum power to detect $\beta_{XIc}$ of $80\%$ $(1-\beta)$, (2) a minimum power to detect differences in $bXId$ of $80\%$ $(1-\beta)$, and (3) a maximum efficiency possible, i.e., less and more balanced sample sizes are preferred.
- That "hiring" one AU judge cost five times $(5x)$ as much as "hiring" an IL judge, while "hiring" one PT judge cost three times $(3x)$ as much as an IL judge.
- Three criteria for judge sample size selection: (1) a minimum confidence of $95\%$ $(1 - \alpha)$ to not reject $\beta_{ZJc} = 0$, (2) a minimum confidence of $95\%$ $(1 - \alpha)$ to not reject differences in $\beta_{ZJd}$ equal to zero, and (3) maximum efficiency, i.e., less and more balanced sample sizes are preferred.
:::
:::

:::

---

## Appendix A - From Design to Sample: Step 2 {style="font-size:80%;"}

::: {.fragment style="font-size:80%;"}
::: {#fig-individual_ss}
![](/figures/sim_individual_sample_size.png){width=100%}

Individual's sample size calculation, considering requirements for Power, Efficiency and Cost.
:::
:::

---

## Appendix A - From Design to Sample: Step 2 {style="font-size:80%;"}

::: {.fragment style="font-size:80%;"}
::: {#fig-judges_ss}
![](/figures/sim_judges_sample_size.png){width=100%}

Judges's sample size calculation, considering requirements for Confidence, Efficiency and Cost.
:::
:::




# Licence {style="font-size:80%;"}

---

## Licence {style="font-size:80%;"}

All the code that is original to this study and not attributed to any other authors is copyrighted by *Jose Manuel Rivera Espejo* and released under the New BSD (3-Clause) License: [https://opensource.org/license/BSD-3-Clause](https://opensource.org/license/BSD-3-Clause)

<!-- ######################################### -->

---

# References {style="font-size:80%;"}

:::{#refs style="font-size:80%;"}

:::
